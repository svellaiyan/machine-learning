{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import cv2\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom PIL import Image\nfrom keras.preprocessing import image as Kimage\nfrom keras.utils import np_utils\nfrom keras.layers import BatchNormalization\nfrom keras.applications.vgg19 import VGG19, preprocess_input\nfrom keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\nfrom keras.layers import Dropout, Flatten, Dense\nfrom keras.models import Sequential\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint  \nfrom keras import optimizers\n\nfrom tqdm import tqdm\nimport pydicom\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\nimport os\nprint(os.listdir(\"../input/\"))\n",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\nUsing TensorFlow backend.\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "['stage_1_detailed_class_info.csv', 'stage_1_sample_submission.csv', 'stage_1_test_images', 'stage_1_train_labels.csv', 'stage_1_train_images', 'GCP Credits Request Link - RSNA.txt']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "5f5a0c205afd2befe1de47d8b14945be4006f2ca"
      },
      "cell_type": "markdown",
      "source": "Below is the list of parameters that can be configured before running the model\n* model_type: String, FC512|FC1024|FC\n* dropout: Probability\n* optimizer_type: String, Adam|SGD|Adadelta default RMS\n* learning_rate: learing rate for optimizer\n* Augmentation_Indicator: Boolean, Implement Augmentaiton or not\n* epochs = Int\n* batch_size = Int\n* transfer_learning = Boolean, Repsective transfer learnign has to be imported and edited in code\n* random_state = random value\n* resize=Boolean\n* input_shape=tuple, Resize shape (256,256,3)\n* rescale=Int\n* sample_ratio = any value 0 to 1, Limiting the input data"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "6c9a54c8bf7b169e13b80df398701dbc3e3dbcfa"
      },
      "cell_type": "code",
      "source": "model_type = 'FC1024'\ndropout = 0.4\noptimizer_type = 'Adam'\nlearning_rate = 1e-4\nAugmentation_Indicator = False\nepochs = 15\nbatch_size = 16\ntransfer_learning = True\nrandom_state = 1607\nresize=True\ninput_shape=(256, 256, 3)\nrescale=100\nsample_ratio = 0.35",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "47f9c1dc255335fdb2df54924d99c20c52cc6401"
      },
      "cell_type": "code",
      "source": "input_shape[0:2]",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "(256, 256)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Load train labels\ninput_data = pd.read_csv(\"../input/stage_1_detailed_class_info.csv\")\ninput_data['img_path'] = '../input/stage_1_train_images/' + input_data['patientId'] + '.dcm'\n\n# Convert class into categorical variable\ninput_data['class'] = pd.Categorical(input_data['class'])\ninput_data['target'] = input_data['class'].cat.codes\n\n# Start with around 1500 - 2000 images \n# This step is not needed when it is going to be trained with full resources \nremove, input_data  = train_test_split(input_data, \n                                test_size=sample_ratio, \n                                random_state=random_state,\n                                stratify=input_data['class'])\n\nprint('Total images taken: {}'.format(input_data.shape[0]))\ntotal_images=input_data.shape[0]\ninput_data.head()",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Total images taken: 10147\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "                                  patientId                         class  \\\n12429  7ed2ee08-c7e0-40c5-afe8-d6327df0a5ee  No Lung Opacity / Not Normal   \n4734   40f55d75-0e89-4392-a32c-84ff66530a4a  No Lung Opacity / Not Normal   \n26939  efc5ae9c-b54b-4cf3-a448-403241ec3e03  No Lung Opacity / Not Normal   \n17405  a67be9a7-9130-4e1b-a7a1-1f137ffddb7d                  Lung Opacity   \n4991   432be6a7-fb99-4eec-827c-deb0310b8967  No Lung Opacity / Not Normal   \n\n                                                img_path  target  \n12429  ../input/stage_1_train_images/7ed2ee08-c7e0-40...       1  \n4734   ../input/stage_1_train_images/40f55d75-0e89-43...       1  \n26939  ../input/stage_1_train_images/efc5ae9c-b54b-4c...       1  \n17405  ../input/stage_1_train_images/a67be9a7-9130-4e...       0  \n4991   ../input/stage_1_train_images/432be6a7-fb99-4e...       1  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>patientId</th>\n      <th>class</th>\n      <th>img_path</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>12429</th>\n      <td>7ed2ee08-c7e0-40c5-afe8-d6327df0a5ee</td>\n      <td>No Lung Opacity / Not Normal</td>\n      <td>../input/stage_1_train_images/7ed2ee08-c7e0-40...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4734</th>\n      <td>40f55d75-0e89-4392-a32c-84ff66530a4a</td>\n      <td>No Lung Opacity / Not Normal</td>\n      <td>../input/stage_1_train_images/40f55d75-0e89-43...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>26939</th>\n      <td>efc5ae9c-b54b-4cf3-a448-403241ec3e03</td>\n      <td>No Lung Opacity / Not Normal</td>\n      <td>../input/stage_1_train_images/efc5ae9c-b54b-4c...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>17405</th>\n      <td>a67be9a7-9130-4e1b-a7a1-1f137ffddb7d</td>\n      <td>Lung Opacity</td>\n      <td>../input/stage_1_train_images/a67be9a7-9130-4e...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4991</th>\n      <td>432be6a7-fb99-4eec-827c-deb0310b8967</td>\n      <td>No Lung Opacity / Not Normal</td>\n      <td>../input/stage_1_train_images/432be6a7-fb99-4e...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "f414df183995601975f4a040261ab904b51e8e17"
      },
      "cell_type": "markdown",
      "source": "Split the data-set into train, valida and test.\nTrain: 72%, Valid: 18%, Test: 10%"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c739373465e3979cef34d8734725c83eaa96555f"
      },
      "cell_type": "code",
      "source": "# Split train and test images\ntrain, test = train_test_split(input_data, \n                                test_size=0.1, \n                                random_state=random_state,\n                                stratify=input_data['class'])\n\n# Split train and validation images\ntrain, valid = train_test_split(train, \n                                test_size=0.2, \n                                random_state=random_state,\n                                stratify=train['class'])\n\nprint('Total train images taken: {}'.format(train.shape[0]))\nprint('Total validation images taken: {}'.format(valid.shape[0]))\nprint('Total test images taken: {}'.format(test.shape[0]))\n",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Total train images taken: 7305\nTotal validation images taken: 1827\nTotal test images taken: 1015\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "483fff30b39bfd8cd593eb1a0b224e97b0136acc"
      },
      "cell_type": "markdown",
      "source": "The below functions help to read an image and convert into tensor required for keras processing"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "687c3481f736c930d4423bd112a2e53f083e2971",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "def load_dicom_image(img_path):\n    \"\"\"\n    This function takes the image path and reads the DICOM image\n    :param img_path: image path\n    :return: pil Image\n    \"\"\"\n    img_arr = pydicom.read_file(img_path).pixel_array\n    img_arr = img_arr/img_arr.max()\n    slice_value = (255*img_arr).clip(0, 255).astype(np.uint8)\n    img = Image.fromarray(slice_value)\n    Kimage.pil_image = img\n    return Kimage.pil_image\n\ndef load_resized_image(img_path):\n    \"\"\"\n    This function converts the image into new shape as set in parameters\n    :param imag_path: image path\n    :return: image array\n    \"\"\"\n    ds = pydicom.dcmread(img_path)\n    resized_image = cv2.resize(ds.pixel_array, input_shape[0:2])\n    resized_image = np.repeat(resized_image[:, :, np.newaxis], 3, axis=2)\n    return resized_image.astype(np.float32)\n\n# Convert 3D tensors to 4D tensors where each 4D tensor is a different image\ndef path_to_tensor(img_path, resize=False):\n    \"\"\"\n    This function reads 2D array\n    :param imag_path: image path\n    :return: 3D Tensor\n    \"\"\"\n    if resize==True:\n        x = load_resized_image(img_path)      \n    else:\n        # Read the dcm image using pydicom\n        img = load_dicom_image(img_path)\n        # convert PIL.Image.Image type to 3D tensor\n        x = Kimage.img_to_array(img)\n        # Since it is a grayscale image convert into three channels\n        x = np.squeeze(np.repeat(x[:, :, np.newaxis], 3, axis=2), axis=3)\n    # convert 3D tensor to 4D tensor with shape and return 4D tensor\n    return np.expand_dims(x, axis=0)\n\ndef paths_to_tensor(img_paths):\n    \"\"\"\n    This function reads 3D arrays and returns the 4D Tensor\n    :param imag_path: image paths of all 3D arrays\n    :return: 4D Tensor\n    \"\"\"\n    list_of_tensors = [preprocess_input(path_to_tensor(img_path, resize)) for img_path in tqdm(img_paths)]\n    return np.vstack(list_of_tensors)\n",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a9c2d47fe98177041fc8146eddabe9181f9b2356"
      },
      "cell_type": "code",
      "source": "# Load all the tensors and re-scale the data if requried\ntrain_tensors = paths_to_tensor(train['img_path'])*rescale\nvalid_tensors = paths_to_tensor(valid['img_path'])*rescale\ntest_tensors = paths_to_tensor(test['img_path'])*rescale\n",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": "100%|██████████| 7305/7305 [01:30<00:00, 80.55it/s]\n100%|██████████| 1827/1827 [00:21<00:00, 83.13it/s]\n100%|██████████| 1015/1015 [00:12<00:00, 83.45it/s]\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e4de2cede9a208738e0822c04fb21cc75b4d13fe",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Load all the targets\ntrain_targets = np_utils.to_categorical(np.array(train['target']), 3)\nvalid_targets = np_utils.to_categorical(np.array(valid['target']), 3)\ntest_targets = np_utils.to_categorical(np.array(test['target']), 3)",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7b086f793831289024a74e93a48adee8f04482fc"
      },
      "cell_type": "code",
      "source": "train_tensors.shape",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "(7305, 256, 256, 3)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "18a64f800b93c85bf0ceb6ec4ef464954f1bad39"
      },
      "cell_type": "markdown",
      "source": "The below step will downlaod the trasnfer learning model from Keras and load the weights. This will be the base model if transfer learning is chose. This model can not be trained due to limitation of resources"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c8dbf2ddf49b947aee0a5a6666149039d98697a9"
      },
      "cell_type": "code",
      "source": "if transfer_learning:\n    # Load VGG/Xception model from keras\n    base_model = VGG19(input_shape=input_shape, weights='imagenet', include_top=False)\n    base_model.trainable = False\n    output_shape_trf_learning=base_model.get_output_shape_at(0)[1:]\n    learning_name = 'VGG19TransferLearning'\n    base_model.summary()\nelse:\n    input_shape=input_shape\n    learning_name = 'OwnCNN'",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n80142336/80134624 [==============================] - 2s 0us/step\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         (None, 256, 256, 3)       0         \n_________________________________________________________________\nblock1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      \n_________________________________________________________________\nblock1_conv2 (Conv2D)        (None, 256, 256, 64)      36928     \n_________________________________________________________________\nblock1_pool (MaxPooling2D)   (None, 128, 128, 64)      0         \n_________________________________________________________________\nblock2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     \n_________________________________________________________________\nblock2_conv2 (Conv2D)        (None, 128, 128, 128)     147584    \n_________________________________________________________________\nblock2_pool (MaxPooling2D)   (None, 64, 64, 128)       0         \n_________________________________________________________________\nblock3_conv1 (Conv2D)        (None, 64, 64, 256)       295168    \n_________________________________________________________________\nblock3_conv2 (Conv2D)        (None, 64, 64, 256)       590080    \n_________________________________________________________________\nblock3_conv3 (Conv2D)        (None, 64, 64, 256)       590080    \n_________________________________________________________________\nblock3_conv4 (Conv2D)        (None, 64, 64, 256)       590080    \n_________________________________________________________________\nblock3_pool (MaxPooling2D)   (None, 32, 32, 256)       0         \n_________________________________________________________________\nblock4_conv1 (Conv2D)        (None, 32, 32, 512)       1180160   \n_________________________________________________________________\nblock4_conv2 (Conv2D)        (None, 32, 32, 512)       2359808   \n_________________________________________________________________\nblock4_conv3 (Conv2D)        (None, 32, 32, 512)       2359808   \n_________________________________________________________________\nblock4_conv4 (Conv2D)        (None, 32, 32, 512)       2359808   \n_________________________________________________________________\nblock4_pool (MaxPooling2D)   (None, 16, 16, 512)       0         \n_________________________________________________________________\nblock5_conv1 (Conv2D)        (None, 16, 16, 512)       2359808   \n_________________________________________________________________\nblock5_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n_________________________________________________________________\nblock5_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n_________________________________________________________________\nblock5_conv4 (Conv2D)        (None, 16, 16, 512)       2359808   \n_________________________________________________________________\nblock5_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n=================================================================\nTotal params: 20,024,384\nTrainable params: 0\nNon-trainable params: 20,024,384\n_________________________________________________________________\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "52bb024cdec69d071daaae78b606d594d4d962d3"
      },
      "cell_type": "code",
      "source": "output_shape_trf_learning",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "(8, 8, 512)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "7a911a8b48d0e5b75a2c0bc2e3f9cdb037c0c773"
      },
      "cell_type": "markdown",
      "source": "Below are differnet experimentations of the top layer and this will be seconf part of the trasnfer learning."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "38d92c81eb6eaa6bb8b04d1279b507bd8d083dbf"
      },
      "cell_type": "code",
      "source": "if model_type == 'FC1024':\n    # Build the final layer of the model\n    model = Sequential()\n\n    model.add(Conv2D(filters=1024, kernel_size=2, padding='same', activation='relu', input_shape=output_shape_trf_learning))\n    model.add(BatchNormalization())\n    model.add(Dropout(dropout))\n    model.add(Conv2D(filters=512, kernel_size=2, padding='same', activation='tanh'))\n    model.add(Dropout(dropout))\n    model.add(MaxPooling2D(pool_size=2))\n    model.add(Conv2D(filters=256, kernel_size=2, padding='same', activation='relu'))\n    model.add(Dropout(dropout))\n    model.add(BatchNormalization())\n    model.add(Conv2D(filters=256, kernel_size=2, padding='same', activation='relu'))\n    model.add(Dropout(dropout))\n    model.add(GlobalAveragePooling2D())\n    model.add(Dense(3, activation='softmax'))\n\n    model.summary()\nelif model_type == 'FC512':\n    model = Sequential()\n\n    model.add(Conv2D(filters=1024, kernel_size=2, padding='same', activation='relu', input_shape=output_shape_trf_learning))\n    model.add(BatchNormalization())\n    model.add(Dropout(dropout))\n    model.add(Conv2D(filters=512, kernel_size=2, padding='same', activation='tanh'))\n    model.add(Dropout(dropout))\n    model.add(MaxPooling2D(pool_size=2))\n    model.add(Conv2D(filters=256, kernel_size=2, padding='same', activation='relu'))\n    model.add(Dropout(dropout))\n    model.add(BatchNormalization())\n    model.add(Dense(256,activation='relu'))\n    model.add(Dropout(dropout))\n    model.add(GlobalAveragePooling2D())\n    model.add(Dense(3, activation='softmax'))\n\n    model.summary()\nelif model_type == 'FC16':\n    model = Sequential()\n\n    model.add(Conv2D(filters=16, kernel_size=2, padding='same', activation='relu', input_shape=output_shape_trf_learning))\n    model.add(MaxPooling2D(pool_size=2))\n    model.add(Dropout(dropout))\n    model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='tanh'))\n    model.add(Dropout(dropout))\n    model.add(MaxPooling2D(pool_size=2))\n    model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n    model.add(Dropout(dropout))\n    model.add(MaxPooling2D(pool_size=2))\n    model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n    model.add(Dropout(dropout))\n    model.add(GlobalAveragePooling2D())\n    model.add(Dense(3, activation='softmax'))\n\n    model.summary()\nelif model_type == 'FCTest':\n    model = Sequential()\n\n    model.add(Dense(1024, activation='relu', input_shape=output_shape_trf_learning))\n    model.add(BatchNormalization())\n    model.add(Dropout(dropout))\n    model.add(Dense(512,activation='tanh'))\n    model.add(Dropout(dropout))\n    model.add(Dense(256, activation='relu'))\n    model.add(Dropout(dropout))\n    model.add(BatchNormalization())\n    model.add(Dense(256,activation='relu'))\n    model.add(Dropout(dropout))\n    model.add(GlobalAveragePooling2D())\n    model.add(Dense(3, activation='softmax'))\n\n    model.summary()\nelse:\n    model = Sequential()\n    model.add(BatchNormalization(input_shape=output_shape_trf_learning))\n    model.add(Dense(256,activation='relu'))\n    model.add(Dropout(dropout))\n    model.add(GlobalAveragePooling2D())\n    model.add(Dense(128,activation='relu'))\n    model.add(Dense(3, activation='softmax'))\n\n    model.summary()",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_1 (Conv2D)            (None, 8, 8, 1024)        2098176   \n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 8, 8, 1024)        4096      \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 8, 8, 1024)        0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 8, 8, 512)         2097664   \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 8, 8, 512)         0         \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 4, 4, 512)         0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 4, 4, 256)         524544    \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 4, 4, 256)         0         \n_________________________________________________________________\nbatch_normalization_2 (Batch (None, 4, 4, 256)         1024      \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 4, 4, 256)         262400    \n_________________________________________________________________\ndropout_4 (Dropout)          (None, 4, 4, 256)         0         \n_________________________________________________________________\nglobal_average_pooling2d_1 ( (None, 256)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 3)                 771       \n=================================================================\nTotal params: 4,988,675\nTrainable params: 4,986,115\nNon-trainable params: 2,560\n_________________________________________________________________\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "19797656c84e9b970897ffa125545e990d590592"
      },
      "cell_type": "code",
      "source": "# Combine pre-trained model and customized final layers\nif transfer_learning:\n    final_model = Sequential(name='Pneumonia Classifier')\n    final_model.add(base_model)\n    final_model.add(model)\nelse:\n    final_model = model\n    \nfinal_model.summary()",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nvgg19 (Model)                (None, 8, 8, 512)         20024384  \n_________________________________________________________________\nsequential_1 (Sequential)    (None, 3)                 4988675   \n=================================================================\nTotal params: 25,013,059\nTrainable params: 4,986,115\nNon-trainable params: 20,026,944\n_________________________________________________________________\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "72ebbfcae094d7e9af04236a4dae413659b5e9ec",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Compile the model\nif optimizer_type == 'SGD':\n    optimizer=optimizers.SGD(lr=learning_rate, momentum=0.9)\nelif optimizer_type == 'Adam':\n    optimizer = optimizers.Adam(lr=learning_rate)\nelif optimizer_type == 'Adadelta':\n    optimizer = optimizers.Adadelta() \nelse:\n    optimizer = optimizers.RMSprop()\n    \nfinal_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['categorical_accuracy'])",
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8abaf90ee134e17ba7d831a2df92938a084cf265"
      },
      "cell_type": "markdown",
      "source": "The next step is Augmentation and it will take place only if it was set to True in in the parameters."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "82486188faee0df46f0510e92dcd893ccb532854",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "if Augmentation_Indicator:\n    # create and configure augmented image generator\n    datagen_train = ImageDataGenerator(\n            rotation_range=25,\n            width_shift_range=0.2,\n            height_shift_range=0.2,\n            shear_range=0.2,\n            zoom_range=0.2,\n            horizontal_flip=True,\n            fill_mode='nearest')\n\n    # create and configure augmented image generator\n    datagen_valid = ImageDataGenerator(\n            rotation_range=25,\n            width_shift_range=0.2,\n            height_shift_range=0.2,\n            shear_range=0.2,\n            zoom_range=0.2,\n            horizontal_flip=True,\n            fill_mode='nearest')\n    # fit augmented image generator on data\n    datagen_train.fit(train_tensors)\n    datagen_valid.fit(valid_tensors)",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "13e51e7aba72f023f49f2afb42531d45ff0e776c",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Make the directory to save the models\nos.mkdir('/kaggle/working/saved-models')",
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9a33c3d260378d959ede385082750d39784ea55e"
      },
      "cell_type": "code",
      "source": "# Create a pattern to do some analysis later to understand how each parameter affects the model's performance.\nif Augmentation_Indicator:\n    model_weights_name = 'weights.best.{}_wAug_{}_{}_{}_{}_{}_{}.hd5'.format(learning_name, model_type, dropout, optimizer_type,learning_rate, epochs, batch_size)\nelse:\n    model_weights_name = 'weights.best.{}_woAug_{}_{}_{}_{}_{}_{}.hd5'.format(learning_name, model_type, dropout, optimizer_type,learning_rate, epochs, batch_size)\nprint(model_weights_name)",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": "weights.best.VGG19TransferLearning_woAug_FC1024_0.4_Adam_0.0001_15_16.hd5\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2a367d60e61c5e790aa5f8dbe4acf2dbf27e495b"
      },
      "cell_type": "code",
      "source": "# Fit the model and save the model with best weights\nfrom keras.callbacks import ModelCheckpoint  \n\ncheckpointer = ModelCheckpoint(filepath='/kaggle/working/saved-models/{}'.format(model_weights_name), \n                               verbose=1, save_best_only=True)\n\nif Augmentation_Indicator:\n    final_model.fit_generator(datagen_train.flow(train_tensors, train_targets, batch_size=batch_size),\n                                                steps_per_epoch=train_tensors.shape[0] // batch_size,\n                                                epochs=epochs, verbose=1, callbacks=[checkpointer],\n                                                validation_data=datagen_valid.flow(valid_tensors, valid_targets, batch_size=batch_size),\n                                                validation_steps=valid_tensors.shape[0] // batch_size)\nelse:\n    final_model.fit(train_tensors, train_targets, \n              validation_data=(valid_tensors, valid_targets),\n              epochs=epochs, batch_size=batch_size, callbacks=[checkpointer], verbose=1)\n\n\n",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Train on 7305 samples, validate on 1827 samples\nEpoch 1/15\n7305/7305 [==============================] - 148s 20ms/step - loss: 0.8115 - categorical_accuracy: 0.6118 - val_loss: 0.7364 - val_categorical_accuracy: 0.6694\n\nEpoch 00001: val_loss improved from inf to 0.73641, saving model to /kaggle/working/saved-models/weights.best.VGG16TransferLearning_woAug_FC1024_0.4_Adam_0.0001_15_16.hd5\nEpoch 2/15\n7305/7305 [==============================] - 144s 20ms/step - loss: 0.7213 - categorical_accuracy: 0.6679 - val_loss: 0.7034 - val_categorical_accuracy: 0.6847\n\nEpoch 00002: val_loss improved from 0.73641 to 0.70342, saving model to /kaggle/working/saved-models/weights.best.VGG16TransferLearning_woAug_FC1024_0.4_Adam_0.0001_15_16.hd5\nEpoch 3/15\n7305/7305 [==============================] - 144s 20ms/step - loss: 0.6790 - categorical_accuracy: 0.6964 - val_loss: 0.7020 - val_categorical_accuracy: 0.6946\n\nEpoch 00003: val_loss improved from 0.70342 to 0.70201, saving model to /kaggle/working/saved-models/weights.best.VGG16TransferLearning_woAug_FC1024_0.4_Adam_0.0001_15_16.hd5\nEpoch 4/15\n7305/7305 [==============================] - 144s 20ms/step - loss: 0.6526 - categorical_accuracy: 0.7051 - val_loss: 0.7721 - val_categorical_accuracy: 0.6475\n\nEpoch 00004: val_loss did not improve\nEpoch 5/15\n7305/7305 [==============================] - 144s 20ms/step - loss: 0.6142 - categorical_accuracy: 0.7265 - val_loss: 0.7034 - val_categorical_accuracy: 0.6749\n\nEpoch 00005: val_loss did not improve\nEpoch 6/15\n7305/7305 [==============================] - 144s 20ms/step - loss: 0.5766 - categorical_accuracy: 0.7435 - val_loss: 0.7429 - val_categorical_accuracy: 0.6683\n\nEpoch 00006: val_loss did not improve\nEpoch 7/15\n7305/7305 [==============================] - 144s 20ms/step - loss: 0.5143 - categorical_accuracy: 0.7730 - val_loss: 0.7418 - val_categorical_accuracy: 0.6924\n\nEpoch 00007: val_loss did not improve\nEpoch 8/15\n7305/7305 [==============================] - 144s 20ms/step - loss: 0.4506 - categorical_accuracy: 0.8086 - val_loss: 0.8861 - val_categorical_accuracy: 0.6836\n\nEpoch 00008: val_loss did not improve\nEpoch 9/15\n7305/7305 [==============================] - 144s 20ms/step - loss: 0.3708 - categorical_accuracy: 0.8487 - val_loss: 1.0107 - val_categorical_accuracy: 0.6136\n\nEpoch 00009: val_loss did not improve\nEpoch 10/15\n7305/7305 [==============================] - 144s 20ms/step - loss: 0.3056 - categorical_accuracy: 0.8790 - val_loss: 1.0169 - val_categorical_accuracy: 0.6793\n\nEpoch 00010: val_loss did not improve\nEpoch 11/15\n7305/7305 [==============================] - 144s 20ms/step - loss: 0.2429 - categorical_accuracy: 0.9046 - val_loss: 1.1426 - val_categorical_accuracy: 0.6727\n\nEpoch 00011: val_loss did not improve\nEpoch 12/15\n7305/7305 [==============================] - 144s 20ms/step - loss: 0.1940 - categorical_accuracy: 0.9247 - val_loss: 1.1740 - val_categorical_accuracy: 0.6656\n\nEpoch 00012: val_loss did not improve\nEpoch 13/15\n7305/7305 [==============================] - 144s 20ms/step - loss: 0.1651 - categorical_accuracy: 0.9366 - val_loss: 1.2858 - val_categorical_accuracy: 0.6689\n\nEpoch 00013: val_loss did not improve\nEpoch 14/15\n7305/7305 [==============================] - 144s 20ms/step - loss: 0.1417 - categorical_accuracy: 0.9477 - val_loss: 1.4900 - val_categorical_accuracy: 0.6530\n\nEpoch 00014: val_loss did not improve\nEpoch 15/15\n7305/7305 [==============================] - 144s 20ms/step - loss: 0.1298 - categorical_accuracy: 0.9500 - val_loss: 1.4063 - val_categorical_accuracy: 0.6661\n\nEpoch 00015: val_loss did not improve\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8eae36666aef41e37b667444e70c7a3cdbb2470e",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Load the model with best weights\nfinal_model.load_weights('/kaggle/working/saved-models/{}'.format(model_weights_name))",
      "execution_count": 19,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9efa70a6ae9653cf102db36f5b864d4103aa96be"
      },
      "cell_type": "code",
      "source": "# Get index of predicted value for each image in test set\npredictions = [np.argmax(final_model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n\n# Report test accuracy\ntest_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Test accuracy: 70.7389%\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1ed3a8b17fdf420c8be1062071e0ab01c53ffd15",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# Print the Classification report\nfrom sklearn.metrics import classification_report\ntarget_names = ['Lung Opacity', 'No Lung Opacity / Not Normal', 'Normal']\nprint(classification_report(np.array(predictions), np.argmax(test_targets, axis=1), target_names = target_names))",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": "                              precision    recall  f1-score   support\n\n                Lung Opacity       0.70      0.71      0.70       312\nNo Lung Opacity / Not Normal       0.60      0.67      0.63       360\n                      Normal       0.87      0.75      0.80       343\n\n                 avg / total       0.72      0.71      0.71      1015\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5843344723a424599641cc20f6c49886f659fc65"
      },
      "cell_type": "code",
      "source": "# Print all the parameters\nprint(learning_name, total_images,input_shape, model_type, dropout, optimizer_type,learning_rate, epochs, batch_size, test_accuracy, rescale)",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": "VGG19TransferLearning 10147 (256, 256, 3) FC1024 0.4 Adam 0.0001 15 16 70.73891625615764 100\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "5ed90fdd50c1bf85fcafd1233701a87f46c78960"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}